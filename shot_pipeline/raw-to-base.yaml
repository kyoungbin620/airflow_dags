apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: raw-to-base
  namespace: airflow
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: 577638362884.dkr.ecr.us-west-2.amazonaws.com/aim/spark:3.5.3-python3.12.2-v4
  imagePullPolicy: Always
  mainApplicationFile: s3a://creatz-airflow-jobs/raw_to_parquet/scripts/run_raw_to_parquet.py
  sparkVersion: 3.5.3
  restartPolicy:
    type: Never
  deps:
    pyFiles:
      - s3a://creatz-airflow-jobs/raw_to_parquet/zips/dependencies.zip
  driver:
    cores: 2
    memory: 6g
    memoryOverhead: 512m
    serviceAccount: airflow-irsa
    labels:
      spark-ui-selector: raw_to_swingdata_daily
    nodeSelector:
      intent: spark
  executor:
    cores: 2
    memory: 2g
    memoryOverhead: 512m
    instances: 4
    nodeSelector:
      intent: spark
  sparkConf:
    spark.driver.cores: "2"
    spark.driver.memory: "6g"
    spark.driver.memoryOverhead: "512m"
    spark.driver.maxResultSize: "1g"
    spark.executor.cores: "2"
    spark.executor.memory: "2g"
    spark.executor.memoryOverhead: "512m"
    spark.dynamicAllocation.enabled: "true"
    spark.dynamicAllocation.minExecutors: "2"
    spark.dynamicAllocation.initialExecutors: "4"
    spark.dynamicAllocation.maxExecutors: "32"
    spark.kubernetes.driver.request.cores: "2"
    spark.kubernetes.driver.limit.cores: "2"
    spark.kubernetes.executor.request.cores: "2"
    spark.kubernetes.executor.limit.cores: "2"
    spark.sql.adaptive.enabled: "true"
    spark.sql.adaptive.coalescePartitions.enabled: "true"
    spark.sql.shuffle.partitions: "128"
    spark.default.parallelism: "128"
    spark.sql.files.maxPartitionBytes: "134217728"
    spark.network.timeout: "21600s"
    spark.sql.broadcastTimeout: "18000s"
    spark.memory.offHeap.enabled: "true"
    spark.memory.offHeap.size: "512m"
    spark.sql.sources.partitionOverwriteMode: "dynamic"
    spark.hadoop.fs.s3a.endpoint: "s3.us-west-2.amazonaws.com"
    spark.hadoop.fs.s3a.endpoint.region: "us-west-2"
    spark.hadoop.fs.s3a.access.style: "PathStyle"
    spark.hadoop.fs.s3a.path.style.access: "true"
    spark.hadoop.fs.s3.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
    spark.hadoop.fs.s3a.aws.credentials.provider: "com.amazonaws.auth.WebIdentityTokenCredentialsProvider"
    spark.kubernetes.namespace: "airflow"
    spark.kubernetes.authenticate.driver.serviceAccountName: "airflow-irsa"
    spark.kubernetes.container.image.pullSecrets: "ecr-pull-secret"
    spark.kubernetes.container.image: "577638362884.dkr.ecr.us-west-2.amazonaws.com/aim/spark:3.5.3-python3.12.2-v4"
    spark.kubernetes.driver.container.image: "577638362884.dkr.ecr.us-west-2.amazonaws.com/aim/spark:3.5.3-python3.12.2-v4"
    spark.kubernetes.file.upload.path: "local:///opt/spark/tmp"
    spark.kubernetes.executor.node.selector.intent: "spark"
    spark.eventLog.enabled: "true"
    spark.eventLog.dir: "s3a://aim-spark/spark-events"
    spark.ui.proxyBase: "/spark-ui/raw_to_swingdata_daily"